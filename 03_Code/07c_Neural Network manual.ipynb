{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network - By Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Resources:\n",
    "- https://www.youtube.com/watch?v=Wo5dMEP_BbI&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3&index=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Store data as dataframe and display the head (from data stored in file)\n",
    "column_names = ['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our',\n",
    "                'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order',\n",
    "                'word_freq_mail', 'word_freq_receive', 'word_freq_will', 'word_freq_people', 'word_freq_report',\n",
    "                'word_freq_addresses', 'word_freq_free', 'word_freq_business', 'word_freq_email', 'word_freq_you',\n",
    "                'word_freq_credit', 'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money', \n",
    "                'word_freq_hp', 'word_freq_hpl', 'word_freq_george','word_freq_650', 'word_freq_lab', 'word_freq_labs',\n",
    "                 'word_freq_telnet','word_freq_857', 'word_freq_data', 'word_freq_415','word_freq_85','word_freq_technology',\n",
    "                'word_freq_1999', 'word_freq_parts','word_freq_pm','word_freq_direct','word_freq_cs','word_freq_meeting',\n",
    "                'word_freq_original','word_freq_project','word_freq_re','word_freq_edu','word_freq_table','word_freq_conference',\n",
    "                \n",
    "                'char_freq_;','char_freq_(','char_freq_[','char_freq_!','char_freq_$','char_freq_#',\n",
    "                \n",
    "                'capital_run_length_average','capital_run_length_longest','capital_run_length_total','spam'\n",
    "                ]\n",
    "\n",
    "filepath = '/Users/ejallen/Desktop/Machine Learning Project/02_Data/spambase/spambase.data'\n",
    "df_spam = pd.read_csv(filepath, header=None, names=column_names)\n",
    "\n",
    "display(df_spam.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into training and testing\n",
    "x = df_spam.drop('spam', axis=1)\n",
    "y = df_spam.iloc[:,-1].to_numpy().ravel()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # activation function\n",
    "class NeuralNetwork():\n",
    "\n",
    "    def activation_sigmoid(self, x):\n",
    "        return(1/(1 + np.exp(-x)))\n",
    "\n",
    "    # Creating the Feed forward neural network\n",
    "    # 1 Input layer(1, 30)\n",
    "    # 1 hidden layer (1, 5)\n",
    "    # 1 output layer(3, 3)\n",
    "\n",
    "    def f_forward(self, x, w1, w2):\n",
    "        # hidden\n",
    "        z1 = x.dot(w1)# input from layer 1 \n",
    "        a1 = self.sigmoid(z1)# out put of layer 2 \n",
    "        \n",
    "        # Output layer\n",
    "        z2 = a1.dot(w2)# input of out layer\n",
    "        a2 = self.sigmoid(z2)# output of out layer\n",
    "        \n",
    "        return(a2)\n",
    "\n",
    "    # initializing the weights randomly\n",
    "    def generate_wt(self,x, y):\n",
    "        l =[]\n",
    "        for i in range(x * y):\n",
    "            l.append(np.random.randn())\n",
    "        return(np.array(l).reshape(x, y))\n",
    "        \n",
    "    # for loss we will be using mean square error(MSE)\n",
    "    def loss_function(self,out, Y):\n",
    "        s =(np.square(out-Y))\n",
    "        s = np.sum(s)/len(y)\n",
    "        return(s)\n",
    "\n",
    "    # Back propagation of error \n",
    "    def back_propropgation(self,x, y, w1, w2, alpha):\n",
    "        \n",
    "        # hidden layer\n",
    "        z1 = x.dot(w1)# input from layer 1 \n",
    "        a1 = self.sigmoid(z1)# output of layer 2 \n",
    "        \n",
    "        # Output layer\n",
    "        z2 = a1.dot(w2)# input of out layer\n",
    "        a2 = self.sigmoid(z2)# output of out layer\n",
    "        # error in output layer\n",
    "        d2 =(a2-y)\n",
    "        d1 = np.multiply((w2.dot((d2.transpose()))).transpose(), \n",
    "                                    (np.multiply(a1, 1-a1)))\n",
    "\n",
    "        # Gradient for w1 and w2\n",
    "        w1_adj = x.transpose().dot(d1)\n",
    "        w2_adj = a1.transpose().dot(d2)\n",
    "        \n",
    "        # Updating parameters\n",
    "        w1 = w1-(alpha*(w1_adj))\n",
    "        w2 = w2-(alpha*(w2_adj))\n",
    "        \n",
    "        return(w1, w2)\n",
    "\n",
    "    def train(self,x, Y, w1, w2, alpha = 0.01, epoch = 10):\n",
    "        acc =[]\n",
    "        losss =[]\n",
    "        for j in range(epoch):\n",
    "            l =[]\n",
    "            for i in range(len(x)):\n",
    "                out = f_forward(x[i], w1, w2)\n",
    "                l.append((loss(out, Y[i])))\n",
    "                w1, w2 = back_prop(x[i], y[i], w1, w2, alpha)\n",
    "            print(\"epochs:\", j + 1, \"======== acc:\", (1-(sum(l)/len(x)))*100) \n",
    "            acc.append((1-(sum(l)/len(x)))*100)\n",
    "            losss.append(sum(l)/len(x))\n",
    "        return(acc, losss, w1, w2)\n",
    "\n",
    "    def predict(self,x, w1, w2):\n",
    "        Out = f_forward(x, w1, w2)\n",
    "        maxm = 0\n",
    "        k = 0\n",
    "        for i in range(len(Out[0])):\n",
    "            if(maxm<Out[0][i]):\n",
    "                maxm = Out[0][i]\n",
    "                k = i\n",
    "        if(k == 0):\n",
    "            print(\"Image is of letter A.\")\n",
    "        elif(k == 1):\n",
    "            print(\"Image is of letter B.\")\n",
    "        else:\n",
    "            print(\"Image is of letter C.\")\n",
    "        plt.imshow(x.reshape(5, 6))\n",
    "        plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_spam.drop('spam', axis=1)\n",
    "y = df_spam.iloc[:,-1].to_numpy().ravel()\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # weights is shaped in this way to avoid doing transpose later \n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        # note, brackets around the tuple are necessary here to avoid typeError\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activiation_ReLu:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class Activation_Softmax:\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probs = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probs\n",
    "\n",
    "class Loss:\n",
    "\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_Categorical_Cross_Entropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples)]\n",
    "        \n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "        \n",
    "        negative_log_likelihoods = - np.log(correct_confidences)\n",
    "\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# categorical cross entropy\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
